## 正则表达式

### `?` - 非贪婪匹配

```python
import re

line = 'aabooooooooobby123'

greedy = r'.*(b.*b).*'  # 贪婪
non_greedy = r'.*?(b.*?b).*'  # 非贪婪

greedy_obj = re.match(greedy, line)
non_greedy_obj = re.match(non_greedy, line)
print(greedy_obj.group(1))  # bb
print(non_greedy_obj.group(1))  # booooooooob
```

### `group`

- 提取括号里的内容

```python
import re

line = 'bobby123'

regex_str = r'((bobby|boobby)123)'
match_str = re.match(regex_str, line)
print('group():{}'.format(match_str.group()))  # group():bobby123
print('group(1):{}'.format(match_str.group(1)))  # group(1):bobby123
print('group(2):{}'.format(match_str.group(2)))  # group(2):bobby
```

- 平级提取

```python
import re

line = 'qq:446404815'

match_obj = re.match(r"(qq):([1-9]\d{4,8})", line)
print(match_obj.group(1))  # qq
print(match_obj.group(2))  # 446404815
```

### `[]`

- `-`区间

```python
regex_str = r'(1[34578][0-9]{9})'
```

- `^`取返

```python
regex_str = r'(1[34578][^s]{9})'
```

- 中括号中包含的点号其他特殊字符不再有特殊含义(仅指代原字符)

```python
import re

line = '17625990687'

regex_str = r'([.*])'
match_str = re.match(regex_str, line)
if match_str:
    print(match_str.group(1))
else:
    print('no match')
    
'''
result: no match
'''
```

### `unicode`编码 - 提取汉字

> `\u4E00-\u9FA5`

```python
import re

line = 'study in 南京大学'

regex_str = r'.*?([\u4E00-\u9FA5]+大学)'
match_obj = re.match(regex_str, line)
if match_obj:
    print(match_obj.group(1))
else:
    print('no match')

'''
result: 南京大学
'''
```

> 打印当前系统的默认编码格式: `sys.getdefaultencoding()`

## `xpath`

![](/home/wangzheng/文档/notes/image/xpath语法.png)

![](/home/wangzheng/文档/notes/image/xpath语法2.png)

![](/home/wangzheng/文档/notes/image/xpath语法3.png)

#### `contains`

```python
a = response.xpath('//div[contains(@class, "tea_txt")]')
```

> 查找属性中包含某几项的标签
>
> contains(x, y)`:
>
> - x: 属性名
> - y: 包含的值

## `css`选择器

![](/home/wangzheng/文档/notes/image/css选择器.png)

![](/home/wangzheng/文档/notes/image/css选择器2.png)

![](/home/wangzheng/文档/notes/image/css选择器3.png)

## `scrapy`

### 基础

#### 创建项目

```shell
scrapy startproject ArticleSpider
```

#### 生成网站模板

```shell
scrapy genspider jobbole blog.jobbole.com
```

#### 启动命令

命令行启动

```shell
scrapy crawl jobbole
```

代码启动

```python
from scrapy.cmdline import execute

execute(['scrapy', 'crawl', 'jobbole'])
```

调试

```shell
scrapy shell http://www.itcast.cn/channel/teacher.shtml
```

### `Request`

> 接收`url`参数并发起请求，并设置回调函数

```python
import re

import scrapy
from urllib import parse
from scrapy.http import Request  # 1.导入模块


class XinlangSpider(scrapy.Spider):
    name = 'xinlang'
    allowed_domains = ['news.sina.com.cn']
    start_urls = ['https://news.sina.com.cn/']

    def parse(self, response):
        try:
            all = response.xpath('//div[@id="syncad_1"]')[0]
            for each in all.xpath('./h1|./p'):
                crawl_title = each.xpath('.//a/text()').extract()[0]
                crawl_url = each.xpath('.//a/@href').extract()[0]
                url = parse.urljoin(response.url, crawl_url)
                # 2.yield Request可以继续进行爬取，第一个参数为解析出来的url，第二个参数为回调函数
                yield Request(url=url, callback=self.parse_detail)
        except Exception as e:
            print(e)

    def parse_detail(self, response):
        try:
            main_content = response.xpath('//div[contains(@class, "main-content")]')
            title = main_content.xpath('./h1[@class="main-title"]/text()').extract()[0]
            date = main_content.xpath('.//span[@class="date"]/text()').extract()[0]
            source = main_content.xpath('.//a[@class="source"]/text()').extract()[0]
            article = main_content.xpath('.//div[@class="article"]').extract()[0]
            print(title, date, source)
        except Exception as e:
            print(e)
```

#### meta

> `Request`参数，传递元数据到子`response`中

```python
# 1.传递
yield Request(url=url, meta={'a': 123}, callback=self.parse_detail)

# 2.取值
a = response.meta['a']
```

#### `urljoin`

> 拼接图片`url`
>
> 有些图片`url`只有路径没有域名，此函数会自动判断并拼接完整的`url`地址

```python
from urllib import parse

url = parse.urljoin(response.url, crawl_url)
```

### `Items`

> 定义字段

```python
class XinlangArticleItem(scrapy.Item):
    title = scrapy.Field()
    date = scrapy.Field()
    source = scrapy.Field()
    article = scrapy.Field()
    img_urls = scrapy.Field()
    img_paths = scrapy.Field()
    img_url_ids = scrapy.Field()
```

#### `Itemloader`使用

1. 修改spider中爬取并保存数据的逻辑

```python
import scrapy
from urllib import parse
from scrapy.http import Request
from scrapy.loader import ItemLoader  # 1.导包


class XinlangSpider(scrapy.Spider):
    name = 'xinlang'
    allowed_domains = ['news.sina.com.cn']
    start_urls = ['https://news.sina.com.cn/']

    def parse(self, response):
        all = response.xpath('//div[@id="syncad_1"]')[0]
        for each in all.xpath('./h1|./p'):
            list_url = each.xpath('.//a/@href').extract()[0]
            url = parse.urljoin(response.url, list_url)
            url_object_id = get_md5(url)
            yield Request(url=url, meta={'url_object_id': url_object_id}, callback=self.parse_detail)

    def parse_detail(self, response):
        article_item = XinlangArticleItem()  # 获取实例化对象
        url_object_id = response.meta['url_object_id']

        img_urls = self.get_img_urls(response)

        # 3.通过item loader加载item，两个必传参数:item对象，response
        item_loader = ItemLoader(item=article_item, response=response)
        item_loader.add_value('url_object_id', url_object_id)
        item_loader.add_xpath('title', '//div[contains(@class, "main-content")]/h1[@class="main-title"]/text()')
        item_loader.add_xpath('date', '//div[contains(@class, "main-content")]//span[@class="date"]/text()')
        item_loader.add_xpath('source', '//div[contains(@class, "main-content")]//a[@class="source"]/text()')
        item_loader.add_xpath('article', '//div[contains(@class, "main-content")]//div[@class="article"]')
        item_loader.add_value('img_urls', img_urls)

        article_item = item_loader.load_item()  # 4.执行item_loader

        yield article_item

    def get_img_urls(self, response):
        img_urls = []
        crawl_urls = response.xpath('//img/@src').extract()
        if not crawl_urls:
            return None
        for each_url in crawl_urls:
            img_url = parse.urljoin(response.url, each_url)
            img_urls.append(img_url)
        return img_urls
```

2. `Items`中修改数据模型

```python
import re
from datetime import datetime

import scrapy
from scrapy.loader.processors import MapCompose, TakeFirst  # 1.导包


def get_date(crawl_date):
    # 参数crawl_date为spider中xpath所提取出来的原始url
    try:
        match_str = r'.*?(\d*?.*日).*'
        date_obj = re.match(match_str, crawl_date)
        date_match = date_obj.group(1)
        date = datetime.strptime(date_match, '%Y年%m月%d日').date()
    except Exception as e:
        date = datetime.now().date()
    return date


class XinlangArticleItem(scrapy.Item):
    title = scrapy.Field(output_processor=TakeFirst())
    date = scrapy.Field(  # 2.添加参数:input_processor,output_processor输入与输出处理器
        input_processor=MapCompose(get_date),  # 提取出的date通过get_date处理后传入到date字段
        output_processor=TakeFirst()  # 取结果列表中的第一项
    )
    source = scrapy.Field(output_processor=TakeFirst())
    article = scrapy.Field(output_processor=TakeFirst())
    img_urls = scrapy.Field()
    img_paths = scrapy.Field()
    url_object_id = scrapy.Field(output_processor=TakeFirst())
# 3.执行完成后走入到pipelines中执行后续逻辑
```

### `pipelines`

> 做数据存储

#### `settings.py`配置

> 数据流经过的管道，冒号后面的数字越小，越先执行

```python
ITEM_PIPELINES = {
   'ArticleSpider.pipelines.ArticlespiderPipeline': 300,
}
```

#### 图片自动下载

`settings.ITEM_PIPELINES`中添加自带的`ImagesPipeline`

```python
ITEM_PIPELINES = {
   'scrapy.pipelines.images.ImagesPipeline': 1
}
```

其他配置

```python
IMAGES_URLS_FIELD = ''  # 指定需要自动下载的图片url字段
IMAGES_STORE = ''  # 图片保存路径
IMAGES_MIN_HEIGHT = 100  # 取最小高度大于100的图片
IMAGES_MIN_WIDTH = 100  # 取最小宽度大于100的图片
```

#### 获取图片的保存路径

创建自定义类，继承自`scrapy.pipelines.images.ImagesPipeline`

```python
from scrapy.pipelines.images import ImagesPipeline

class ArticleImagePipeline(ImagesPipeline):

    def item_completed(self, results, item, info):
        for ok, value in results:
            if value:
                image_path = value['path']
                item['img_paths'] = image_path  # 将获取到的图片路径加入到字段中
            return item
```

> `pipeline`函数执行完成后需要返回`item`，会自动传入到下一个`pipeline`中

#### 获取`url`的`md5`值

```python
import hashlib


def get_md5(url):
    if isinstance(url, str):
        url = url.encode('utf-8')
    m = hashlib.md5()
    m.update(url)
    return m.hexdigest()
```

#### 保存数据到文件

> `close_spider`在爬虫关闭的时候执行一次

- 方法1

1. `codecs`打开文件，防止编码异常

```python
import codecs

file = codecs.open('article.json', 'w', encoding='utf-8')
self.file.close()
```

2. 自定义`pipeline`用于保存数据到文件

```python
class JsonWithEncodingPipeline(object):
    def __init__(self):
        self.file = codecs.open('article.json', 'w', encoding='utf-8')

    def process_item(self, item, spider):
        lines = json.dumps(dict(item), ensure_ascii=False) + '\n'
        self.file.write(lines)
        return item

    def close_spider(self, spider):
        self.file.close()
```

3. 在`settings`中加入

```python
ITEM_PIPELINES = {
   'ArticleSpider.pipelines.ArticlespiderPipeline': 300,
   'ArticleSpider.pipelines.ArticleImagePipeline': 1,
   'ArticleSpider.pipelines.JsonWithEncodingPipeline': 2  # 第二个执行
}
```

- 方法2：使用`scrapy`自带的`pipeline`保存

```python
from scrapy.exporters import JsonItemExporter

class JsonExporterPipeline(object):

    def __init__(self):
        self.file = open('/home/wangzheng/project/ArticleSpider/article_exporter.json', 'wb')
        self.exporter = JsonItemExporter(self.file, ensure_ascii=False)
        self.exporter.start_exporting()

    def close_spider(self, spider):
        self.exporter.finish_exporting()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
```

> 还支持
>
> ```
> ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',
>            'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter',
>            'JsonItemExporter', 'MarshalItemExporter']
> ```

#### 保存数据到`Mysql`

建库建表

```mysql
-- 建库
CREATE DATABASE `article_spider` CHARSET='utf8';

-- 建表
CREATE TABLE `article`(
	url_object_id varchar(50) primary key not null,
	title varchar(200) not null,
	date date,
	source varchar(200) not null,
	article longtext,
	img_urls varchar(300),
	img_paths varchar(200)
);
```

##### 方法1: 自定义`pipeline`

```python
class MysqlPipeline(object):
    def __init__(self):
        self.conn = pymysql.connect(host='39.108.113.168', port=3306, user='wang', password='123',
                                    database='article_spider', charset='utf8', use_unicode=True)
        self.cursor = self.conn.cursor()

    def process_item(self, item, spider):
        insert_sql = '''INSERT INTO article(url_object_id, title, date, source, article, img_urls, img_paths)
        VALUES(%s, %s, %s, %s, %s, %s, %s);'''
        self.cursor.execute(insert_sql, (item['url_object_id'], item['title'], item['date'],
                                         item['source'], item['article'], ','.join(item['img_urls']),
                                         ','.join(item['img_paths'])))
        self.conn.commit()
        return item

    def close_spider(self, spider):
        self.cursor.close()
        self.conn.close()
```

##### 方法2: 异步插入(twisted)

> `twisted`框架为关系型数据库提供异步操作
>
> `from_settings`在初始化时自动执行
>
> 把游标cursor作为参数传入直接使用，db_pool连接池在执行的时候会自动获取

```python
from twisted.enterprise import adbapi

class MysqlTwistedPipeline(object):

    def __init__(self, db_pool):
        self.db_pool = db_pool

    @classmethod
    def from_settings(cls, settings):
        # from_settings在初始化时自动执行
        dbparams = dict(
            host=settings['MYSQL_HOST'],
            port=settings['MYSQL_PORT'],
            user=settings['MYSQL_USER'],
            password=settings['MYSQL_PASSWORD'],
            database=settings['MYSQL_DATABASE'],
            cursorclass=pymysql.cursors.DictCursor,
            use_unicode=True
        )
        db_pool = adbapi.ConnectionPool('pymysql', **dbparams)  # 建立连接池

        return cls(db_pool)

    def process_item(self, item, spider):
        search = self.db_pool.runInteraction(self.do_insert, item)  # 参数1：要执行的函数 参数2：item
        search.addErrback(self.handle_error)  # 错误回调

    def do_insert(self, cursor, item):
        # 把游标cursor作为参数传入直接使用，db_pool连接池在执行的时候会自动获取
        insert_sql = '''INSERT INTO article(url_object_id, title, date, source, article, img_urls, img_paths)
                VALUES(%s, %s, %s, %s, %s, %s, %s);'''
        cursor.execute(insert_sql, (item['url_object_id'], item['title'], item['date'],
                                    item['source'], item['article'], ','.join(item['img_urls']),
                                    ','.join(item['img_paths'])))

    def handle_error(self, falure):
        print(falure)
```

#### `from_settings`读取配置参数

> 定义类方法`from_settings`，会在初始化时自动执行 

```python
class MysqlTwistedPipeline(object):

    @classmethod
    def from_settings(cls, settings):
        host = settings['MYSQL_HOST']
        pass
```