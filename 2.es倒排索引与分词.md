### 分词器

![](/home/wangzheng/图片/2019-12-03 00-11-08屏幕截图.png)

> 顺序： `Character Filters` ==> `Tokenizer` ==> `Token Filters`

### `Analyze API`

1.直接测试文本

```shell
POST _analyze
{
"analyzer": "standard",
"text": "hello world"
}
```

> `      "start_offset" : 0,  "end_offset" : 5,`起始偏移和结束偏移
>
> `      "position" : 0`分词位置，在所有分词结果中的位置

2.指明索引字段测试

```shell
POST test_index/_analyze
{
"field": "username",
"text": "hello world"
}
```

> 不需要提前知道该字段的分词器使用的是什么，直接拿来测试

3.自定义分词器

```shell
POST _analyze
{
"tokenizer": "standard",
"filter": ["lowercase"],
"text": "Hello World"
}
```

> 自定义`Character Filter`, `Tokenizer`, `Token Filters`三部分来测试分词结果

### `es`自带分词器

`standard`, `simple`, `whitespace`, `stop`, `keyword`, `pattern`, `language`

```shell
POST _analyze
{
"analyzer": "whitespace",
"text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone"
}
```

![](/home/wangzheng/文档/notes/image/Standard Analyzer.png)

![](/home/wangzheng/文档/notes/image/Simple Analyzer.png)

![](/home/wangzheng/文档/notes/image/Whitespace Analyzer.png)

![](/home/wangzheng/文档/notes/image/Stop Analyzer.png)

![](/home/wangzheng/文档/notes/image/Keyword Analyzer.png)

![](/home/wangzheng/文档/notes/image/Pattern Analyzer.png)

![](/home/wangzheng/文档/notes/image/Language Analyzer.png)

### 自定义分词

#### `character filter`

> 在`tokenizer`之前对原始文本进行处理，比如增加、删除或替换字符等
>
> 自带的如下：
>
> `html strip`去除`html`标签和转换`html`实体
>
> `mapping`进行字符替换操作
>
> `pattern replace`进行正则匹配替换

```shell
POST _analyze
{
	"tokenizer": "keyword",
	"char_filter": ["html_strip"],
	"text": "<p>I&apos;m so<b>happy</b>!</p>"
}
```

#### `tokenizer`

> 将原始文本按照一定规则切分成单词
>
> 自带的如下：
>
> `standard`按照单词进行分割
>
> `letter`按照非字符类进行分割
>
> `whitespace`按照空格进行分割
>
> `UAX URL Email`类似`standard`，但不会分割邮箱和`url`
>
> `NGram`和`Edge NGram`连词分割 --> 做自动提示时常用
>
> `Path Hierarchy`按照文件路径进行分割

```shell
POST _analyze
{
	"tokenizer": "path_hierarchy",
	"text": "/one/two/three"
}
```

#### `token filters`

> 对于`tokenizer`输出的单词`term`进行增加、删除、修改等操作
>
> 自带的如下：
>
> `lowercase`将所有`term`转换为小写
>
> `stop`删除`stop words`
>
> `NGram`和`Edge NGram`连词分割
>
> `Synonym`添加近义词的`term`

```shell
POST _analyze
{
	"text": "a Hello,world!",
	"tokenizer": "standard",
	"filter": [
		"stop",
		"lowercase",
		{
			"type": "ngram",
			"min_gram": 4,
			"max_gram": 4
		}
	]
}
```

#### 自定义分词`api`

```shell
PUT test_index
{
	"settings": {
		"analysis": {
			"analyzer": {
				"my_custom_analyzer": {
					"type": "custom",
					"tokenizer": "standard",
					"char_filter": [
						"html_strip"
					],
					"filter": [
						"lowercase",
						"asciifolding"
					]
				}
			}
		}
	}
}
```

测试效果

```shell
POST test_index/_analyze
{
	"analyzer": "my_custom_analyzer",
	"text": "Is this <b>a box</b>?"
}
```

```shell
PUT test_index2
{
	"settings": {
		"analysis": {
			"analyzer": {
				"my_custom_analyzer": {
					"type": "custom",
					"char_filter": [
						"emoticons"
					],
					"tokenizer": "punctuation",
					"filter": [
						"lowercase",
						"english_stop"
					]
				}
			},
			"tokenizer": {
				"punctuation": {
					"type": "pattern",
					"pattern": "[.,!?]"
				}
			},
			"char_filter": {
				"emoticons": {
					"type": "mapping",
					"mappings": [
						":) => _happy_",
						":( => _sad_"
					]
				}
			},
			"filter": {
				"english_stop": {
					"type": "stop",
					"stopwords": "_english_"
				}
			}
		}
	}
}
```

测试效果

```shell
POST test_index2/_analyze
{
	"analyzer": "my_custom_analyzer",
	"text": "I'm a :) person, and you?"
}
```

### 分词的使用场景

> 分词在两种情况下会被使用:
>
> 1.创建或更新文档时，会对相应的文档进行分词处理
>
> 2.查询时，会对查询语句进行分词

#### 索引时分词

> 通过在`mapping`中指定`analyzer`参数实现

```shell
PUT test_index
{
    "mappings": {
            "properties": {
            "title": {
                "type": "text",
                "analyzer": "whitespace"
            }
        }
    }
}
```

#### 查询时分词

1. 查询的时候通过`analyzer`指定分词器

```shell
POST test_index/_search
{
	"query": {
		"match": {
			"message": {
				"query": "hello",
				"analyzer": "standard"
			}
		}
	}
}
```

2. 通过`mapping`设置`search_analyzer`实现

> `analyzer`是索引时分词，`search_analyzer`是查询时分词，一般不需要特别指定查询时分词器(不存在时会默认使用索引时分词器`analyzer`)

```shell
PUT test_index
{
	"mappings": {
		"properties": {
			"title": {
				"type": "text",
				"analyzer": "whitespace",
				"search_analyzer": "standard"
			}
		}
	}
}
```

### 分词的使用建议

> 1. 明确是否需要分词，不需要分词的字段就将`type`设置为`keyword`，可以节省空间和提高写性能
>
> 2. 善用`_analyze`，查看文档的具体分词结果

### 官方文档

> 本章节对应的文档为`analyzer`，地址`https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html`

